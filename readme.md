## Resource page for the Learner Corpus Workshop at SLRF 2019

This page includes important datasets and other related material for Kris' Learner Corpus Workshop at SLRF 2019 (September 19)

### Learner Corpus
The learner corpus in this workshop is a sample of the written section of the <a href="http://language.sakura.ne.jp/icnale/" target="_blank">ICNALE corpus</a> (Ishikawa, 2013), which is a cross-sectional learner corpus with a substantial amount of participant metadata.

ICNALE includes writing samples from 500 L2 learners of English. During corpus collection, each participant wrote two argumentative essays (one in relation to smoking in public places and another in relation to whether university students should get part time jobs). Participants also took a [Vocabulary Size Test](https://www.lextutor.ca/tests/vst/index.php?mode=test) (VST; a passive vocabulary size test) and reported standardized language proficiency test scores (e.g., TOEFL, TOEIC, IELTS, etc.), which were converted to CEFR levels.

The sample compiled for this workshop (ICNALE 500) includes both writing samples for 500 participants (both essay responses from each participant are included in a single file) across a range of nationalities (these function as an imperfect proxy for L1). In the larger corpus, some participants failed to report a standardized test score, and the corpus compilers estimated their language proficiency using their VST score. However, in this sample, all proficiency scores are based on standardized test scores. Additionally, there are a number of characters in the original corpus that can cause problems with some text analysis programs. These characters have been removed from the ICNALE 500 texts.

[Click here to download the ICNALE 500 corpus](https://kristopherkyle.github.io/LCR_SLRF_2019/data/ICNALE_500_merged_clean.zip) (This also includes a subset of the metadata)

### Linguistic Complexity Indices

All linguistic complexity indices were computed by one of three tools that are freely available on [www.linguisticanalysistools.org](https://www.linguisticanalysistools.org), namely [TAALES](https://www.linguisticanalysistools.org/taales.html), [TAALED](https://www.linguisticanalysistools.org/taaled.html), and [TAASSC](https://www.linguisticanalysistools.org/taassc.html). Full descriptions of these tools can be found in the documents on their related pages and in a few [published papers](http://www.kristopherkyle.com/publicationsgrants.html). A small (but useful!) sample of these indices are briefly described below.

To download a spreadsheet that includes the ICNALE 500 metadata and sores for each of the indices, [click here]()

#### Measuring Lexical Sophistication (TAALES)
The Tool for the Automatic Analysis of Lexical Sophistication (TAALES) measures over 400 indices related to lexical sophistication (i.e., the relative difficulty of learning and/or using a particular word, see also the related definition of Read (2000)). It is our stance that lexical sophistication is a multifaceted construct and should be measured as such (see, e.g., Eguchi & Kyle, under review; Kim, Crossley, & Kyle, 2018, Kyle & Crossley, 2015; Kyle, Crossley, & Berger, 2018, inter alia). Below are brief descriptions of indices that represent a few of these facets.

##### Frequency
Reference corpus frequency is a time-tested method of measuring how sophisticated a word is (see Laufer, 1994; Laufer & Nation, 1995, inter alia). Of course, the corpus from whence frequency figures are derived will likely have an important impact on how well they measure sophistication in a particular language use domain.

Today, we will look at indices that consider content words (nouns, verbs, adjectives, and most adverbs) from a corpus of sitcom and movie subtitles (SUBTLEXus; Brysbaert & New, 2009) and from the magazine section of the Corpus of Contemporary American English (COCA; Davies, 2010). Each index is calculated using logarithmically transformed frequency figures, which helps account for the Zipfian distribution of frequency data.
